{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "import mltools as ml\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('./X_train.txt', delimiter=',')\n",
    "Y = np.genfromtxt('./Y_train.txt', delimiter=',')\n",
    "X_test = np.genfromtxt('./X_test.txt', delimiter=',')\n",
    "X,Y = ml.shuffleData(X,Y)\n",
    "Xtr,Xva,Ytr,Yva = ml.splitData(X,Y,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=0, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "\n",
      "Training error: 0.41530447278606075\n",
      "Validation error: 0.4288793103448276\n",
      "ROC: 0.5940327975471522\n"
     ]
    }
   ],
   "source": [
    "#Logistic repressions\n",
    "logistic_regression =linear_model.LogisticRegression(random_state=0, solver='liblinear')\n",
    "logistic_regression.fit(Xtr, Ytr)\n",
    "print(logistic_regression)\n",
    "print(\"\")\n",
    "\n",
    "logistic_regression.predict(Xtr)\n",
    "logistic_regression.predict_proba(Xtr)\n",
    "print \"Training error:\", (1 - logistic_regression.score(Xtr, Ytr))\n",
    "\n",
    "logistic_regression.predict(Xva)\n",
    "logistic_regression.predict_proba(Xva)\n",
    "print \"Validation error:\", (1 - logistic_regression.score(Xva, Yva))\n",
    "\n",
    "logistic_regression_roc = metrics.roc_auc_score(Yva, logistic_regression.predict_proba(Xva)[:,1])\n",
    "print(\"ROC: \" + str(logistic_regression_roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=16, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "Training error: 0.38099514999101847\n",
      "Validation error: 0.45905172413793105\n",
      "ROC: 0.5520492660039023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=16)\n",
    "knn.fit(Xtr, Ytr)\n",
    "knn_roc = metrics.roc_auc_score(Yva, knn.predict_proba(Xva)[:,1])\n",
    "print(knn)\n",
    "print(\"\")\n",
    "print(\"Training error: \" + str(1 - knn.score(Xtr, Ytr)))\n",
    "print(\"Validation error: \" + str(1 - knn.score(Xva, Yva)))\n",
    "print(\"ROC: \" + str(knn_roc))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=30, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=700, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\n",
      "Training error: 0.19328183941081367\n",
      "Validation error: 0.3604525862068966\n",
      "ROC: 0.7183708073957075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "random_forest = ensemble.RandomForestClassifier(n_estimators=700, min_samples_leaf=30)\n",
    "random_forest.fit(Xtr,Ytr)\n",
    "random_forest_roc = metrics.roc_auc_score(Yva, random_forest.predict_proba(Xva)[:,1])\n",
    "print(random_forest)\n",
    "print(\"\")\n",
    "print(\"Training error: \" + str(1 - random_forest.score(Xtr, Ytr)))\n",
    "print(\"Validation error: \" + str(1 - random_forest.score(Xva, Yva)))\n",
    "print(\"ROC: \" + str(random_forest_roc))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=8,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=70,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "\n",
      "Training error: 0.25076342733968027\n",
      "Validation error: 0.3410560344827587\n",
      "Roc: 0.7282861423394964\n"
     ]
    }
   ],
   "source": [
    "gradient_boost = ensemble.GradientBoostingClassifier(learning_rate=0.1, n_estimators=70, max_depth=3, max_leaf_nodes=8)\n",
    "gradient_boost.fit(Xtr,Ytr)\n",
    "gradient_boost_roc = metrics.roc_auc_score(Yva, gradient_boost.predict_proba(Xva)[:,1])\n",
    "print(gradient_boost)\n",
    "print(\"\")\n",
    "print(\"Training error: \" + str(1-(gradient_boost.score(Xtr, Ytr))))\n",
    "print(\"Validation error: \" + str(1-(gradient_boost.score(Xva, Yva))))\n",
    "print(\"Roc: \" + str(gradient_boost_roc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since given these parameter optimizations, with their validation it can be said that the Gradient Boosting algorithm would yield the best output, followed by Random Forest algorithm, Logistic regression, and K-Nearest Neighbor. Based on the ouputs we should add more weight to Gradient Boosting when combining the different classifers and less weight to K-Nearest Neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [5, 4, 72, 19]\n"
     ]
    }
   ],
   "source": [
    "weights = {gradient_boost: 72,\n",
    "          random_forest: 19,\n",
    "          logistic_regression: 5,\n",
    "          knn: 4}\n",
    "print(\"Weights: \" + str(weights.values()))\n",
    "yhat_list = []\n",
    "for classifer, weight in weights.items():\n",
    "    yhat = classifer.predict_proba(X_test)[:,1]\n",
    "    for i in range(weight):\n",
    "        yhat_list.append(yhat)\n",
    "yhat_average = np.mean(np.array(yhat_list), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Y_submit.txt',np.vstack( (np.arange(len(yhat_average)) , yhat_average) ).T,\n",
    "           '%d, %.2f',header='Id,Predicted',comments='',delimiter=',');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
